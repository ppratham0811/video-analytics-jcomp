{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4Yqme7EwZYi",
        "outputId": "3653bf82-5ec4-47e9-c799-28d671f3ec0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "print(os.getcwd())\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th6K4hhoO24H"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "-nMbLE09PYtf",
        "outputId": "9ed0c8af-df1f-4bdd-ef0c-6feec2920922"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f51690e0-a5f0-4ddd-a1b8-b7b190c23c03\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f51690e0-a5f0-4ddd-a1b8-b7b190c23c03\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"prathameshpotabatti\",\"key\":\"78acef74d5d6994837e54738cb8d10b7\"}'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC42hZTxPhm9"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ./kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaZKCGw9PlKO",
        "outputId": "7b5cafaf-caf0-443f-967b-7d4d26fdc1c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading license-plate-text-recognition-dataset.zip to /content\n",
            " 95% 86.0M/90.8M [00:00<00:00, 204MB/s]\n",
            "100% 90.8M/90.8M [00:00<00:00, 207MB/s]\n"
          ]
        }
      ],
      "source": [
        "! kaggle datasets download -d nickyazdani/license-plate-text-recognition-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF2jXOiNQoM2"
      },
      "outputs": [],
      "source": [
        "zip_ref = zipfile.ZipFile('license-plate-text-recognition-dataset.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waWSA8RMYCvh",
        "outputId": "49c1f58e-8667-4529-a124-4f2fd1e9463b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ],
      "source": [
        "! pip install split-folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R23DLB3HRQzk",
        "outputId": "0c221164-f2ae-481b-f0f0-baa331b9dc17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copying files: 20000 files [00:03, 5662.04 files/s]\n"
          ]
        }
      ],
      "source": [
        "import splitfolders\n",
        "\n",
        "splitfolders.ratio(\"cropped_lps/\", output=\"output\", seed=1337, ratio=(.8, .2), group_prefix=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYWK7COuYNlA"
      },
      "outputs": [],
      "source": [
        "train_data = f\"{os.getcwd()}/output/train/\"\n",
        "val_data = f\"{os.getcwd()}/output/val/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSikIOQAcTgg"
      },
      "outputs": [],
      "source": [
        "letters = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NDkhVIdlnhlH",
        "outputId": "9c7484ad-604c-4ea5-f106-16413b8e0d2c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'V95246'"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"lpr.csv\")\n",
        "df.iloc[1][\"labels\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow998DNxhUGB"
      },
      "outputs": [],
      "source": [
        "# def reshapeAndNormalize(w, h, img):\n",
        "#     if w < 32:\n",
        "#         add_zeros = np.ones((32-w, h))*255\n",
        "#         img = np.concatenate((img, add_zeros))\n",
        "\n",
        "#     if h < 128:\n",
        "#         add_zeros = np.ones((32, 128-h))*255\n",
        "#         img = np.concatenate((img, add_zeros), axis=1)\n",
        "#     img = np.expand_dims(img , axis = 2)\n",
        "      \n",
        "#     # Normalize each image\n",
        "#     img = img/255.\n",
        "\n",
        "#     return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFWva7jOksEM"
      },
      "outputs": [],
      "source": [
        "import fnmatch\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "char_list = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
        "\n",
        "def encode_to_labels(txt):\n",
        "    # encoding each output word into digits\n",
        "    dig_lst = []\n",
        "    for index, char in enumerate(txt):\n",
        "        try:\n",
        "            dig_lst.append(char_list.index(char))\n",
        "        except:\n",
        "            print(char)\n",
        "        \n",
        "    return dig_lst\n",
        "\n",
        "# lists for training dataset\n",
        "training_img = []\n",
        "training_txt = []\n",
        "train_input_length = []\n",
        "train_label_length = []\n",
        "orig_txt = []\n",
        " \n",
        "#lists for validation dataset\n",
        "valid_img = []\n",
        "valid_txt = []\n",
        "valid_input_length = []\n",
        "valid_label_length = []\n",
        "valid_orig_txt = []\n",
        "\n",
        "path = f\"{os.getcwd()}/cropped_lps/\"\n",
        "max_label_len = 10\n",
        "i = 1 \n",
        "flag = 0\n",
        "\n",
        "for root, dirnames, filenames in os.walk(path):\n",
        " \n",
        "    for f_name in fnmatch.filter(filenames, '*.jpg'):\n",
        "        # read input image and convert into gray scale image\n",
        "        img = cv2.cvtColor(cv2.imread(os.path.join(root, f_name)), cv2.COLOR_BGR2GRAY)   \n",
        " \n",
        "        # convert each image of shape (32, 128, 1)\n",
        "        w, h = img.shape\n",
        "        if h > 128 or w > 32:\n",
        "            continue\n",
        "        if w < 32:\n",
        "            add_zeros = np.ones((32-w, h))*255\n",
        "            img = np.concatenate((img, add_zeros))\n",
        " \n",
        "        if h < 128:\n",
        "            add_zeros = np.ones((32, 128-h))*255\n",
        "            img = np.concatenate((img, add_zeros), axis=1)\n",
        "        img = np.expand_dims(img , axis = 2)\n",
        "        \n",
        "        # Normalize each image\n",
        "        img = img/255.\n",
        "\n",
        "        # split the 20000 data into validation and training dataset as 20% and 80% respectively\n",
        "        if i%5 == 0:\n",
        "            valid_orig_txt.append(df.iloc[i][\"labels\"])\n",
        "            valid_label_length.append(len(df.iloc[i][\"labels\"]))\n",
        "            valid_input_length.append(10)\n",
        "            valid_img.append(img)\n",
        "            valid_txt.append(encode_to_labels(df.iloc[i][\"labels\"]))\n",
        "        else:\n",
        "            orig_txt.append(df.iloc[i][\"labels\"])   \n",
        "            train_label_length.append(len(df.iloc[i][\"labels\"]))\n",
        "            train_input_length.append(10)\n",
        "            training_img.append(img)\n",
        "            training_txt.append(encode_to_labels(df.iloc[i][\"labels\"]))\n",
        "        \n",
        "        # break the loop if total data is 20000\n",
        "        if i == 20000:\n",
        "            flag = 1\n",
        "            break\n",
        "        i+=1\n",
        "    if flag == 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrNpjLc-cbaP"
      },
      "outputs": [],
      "source": [
        "img_w = 32\n",
        "img_h = 128\n",
        "img_c = 1\n",
        "num_classes = len(char_list) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j12Aziraso2"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.layers import Dense, Input, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, Reshape, Lambda, MaxPool2D\n",
        "from keras.layers import BatchNormalization, Dropout, LSTM, Bidirectional\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulHYeztFhs8b"
      },
      "outputs": [],
      "source": [
        "def ctc_loss_function(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        "    return keras.backend.ctc_batch_cost(labels, y_pred, input_length, label_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVAp_fDbY27g"
      },
      "outputs": [],
      "source": [
        "def Image_text_recogniser_model(stage=\"train\",drop_out_rate=0.35):\n",
        "    \"\"\"\n",
        "    Builds the model by taking in the stage variable which specifes the stage,\n",
        "    if the stage is training: model takes inputs required for computing ctc_batch_cost function\n",
        "    else : model takes input as images which is used for prediction\n",
        "    \"\"\"\n",
        "    \n",
        "    if keras.backend.image_data_format() == 'channels_first':\n",
        "        input_shape = (1, img_w, img_h)\n",
        "    else:\n",
        "        input_shape = (img_w, img_h, 1)\n",
        "        \n",
        "    model_input=Input(shape=input_shape,name='img_input',dtype='float32')\n",
        "\n",
        "    conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(model_input)\n",
        "    pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
        "    \n",
        "    conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
        "    pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        "    \n",
        "    conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
        "    \n",
        "    conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
        "    pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        "    \n",
        "    conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
        "    batch_norm_5 = BatchNormalization()(conv_5)\n",
        "    \n",
        "    conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
        "    batch_norm_6 = BatchNormalization()(conv_6)\n",
        "    pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        "    \n",
        "    conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
        "\n",
        "    squeezed = Lambda(lambda x: keras.backend.squeeze(x, 1))(conv_7)\n",
        "    # RNN layer\n",
        "    model=Bidirectional(LSTM(128, return_sequences=True, kernel_initializer='he_normal'), merge_mode='sum')(squeezed)\n",
        "    model=Bidirectional(LSTM(128, return_sequences=True, kernel_initializer='he_normal'), merge_mode='concat')(model)\n",
        "\n",
        "    # transforms RNN output to character activations:\n",
        "    model = Dense(num_classes, kernel_initializer='he_normal',name='dense2')(model)\n",
        "    y_pred = Activation('softmax', name='softmax')(model)\n",
        "\n",
        "    labels = Input(name='ground_truth_labels', shape=[max_label_len], dtype='float32') \n",
        "    input_length = Input(name='input_length', shape=[1], dtype='int64') \n",
        "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "    #CTC loss function\n",
        "    loss_out = Lambda(ctc_loss_function, output_shape=(1,),name='ctc')([y_pred, labels, input_length, label_length])\n",
        "\n",
        "    if stage=='train':\n",
        "        return model_input,y_pred,keras.Model(inputs=[model_input, labels, input_length, label_length], outputs=loss_out)\n",
        "    else:\n",
        "        return keras.Model(inputs=[model_input], outputs=y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXVzi7q2md_A"
      },
      "outputs": [],
      "source": [
        "model_input, y_pred, crnn = Image_text_recogniser_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIIKTdOI6AMY",
        "outputId": "0509032d-8688-4244-b6e2-8da16eada57c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " img_input (InputLayer)         [(None, 32, 128, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 32, 128, 64)  640         ['img_input[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling2d_36 (MaxPooling2D  (None, 16, 64, 64)  0           ['conv2d_63[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 16, 64, 128)  73856       ['max_pooling2d_36[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling2d_37 (MaxPooling2D  (None, 8, 32, 128)  0           ['conv2d_64[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_65 (Conv2D)             (None, 8, 32, 256)   295168      ['max_pooling2d_37[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_66 (Conv2D)             (None, 8, 32, 256)   590080      ['conv2d_65[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling2d_38 (MaxPooling2D  (None, 4, 32, 256)  0           ['conv2d_66[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_67 (Conv2D)             (None, 4, 32, 512)   1180160     ['max_pooling2d_38[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 4, 32, 512)  2048        ['conv2d_67[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_68 (Conv2D)             (None, 4, 32, 512)   2359808     ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 4, 32, 512)  2048        ['conv2d_68[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " max_pooling2d_39 (MaxPooling2D  (None, 2, 32, 512)  0           ['batch_normalization_32[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_69 (Conv2D)             (None, 1, 31, 512)   1049088     ['max_pooling2d_39[0][0]']       \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 31, 512)      0           ['conv2d_69[0][0]']              \n",
            "                                                                                                  \n",
            " bidirectional_18 (Bidirectiona  (None, 31, 128)     656384      ['lambda_11[0][0]']              \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " bidirectional_19 (Bidirectiona  (None, 31, 256)     263168      ['bidirectional_18[0][0]']       \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " dense2 (Dense)                 (None, 31, 37)       9509        ['bidirectional_19[0][0]']       \n",
            "                                                                                                  \n",
            " softmax (Activation)           (None, 31, 37)       0           ['dense2[0][0]']                 \n",
            "                                                                                                  \n",
            " ground_truth_labels (InputLaye  [(None, 10)]        0           []                               \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " input_length (InputLayer)      [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " label_length (InputLayer)      [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " ctc (Lambda)                   (None, 1)            0           ['softmax[0][0]',                \n",
            "                                                                  'ground_truth_labels[0][0]',    \n",
            "                                                                  'input_length[0][0]',           \n",
            "                                                                  'label_length[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,481,957\n",
            "Trainable params: 6,479,909\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "crnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggt5crgcmklo"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "crnn.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')\n",
        "\n",
        "filepath=\"best_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "callbacks_list = [checkpoint]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOkofo5FocEH"
      },
      "outputs": [],
      "source": [
        "training_img = np.array(training_img)\n",
        "train_input_length = np.array(train_input_length)\n",
        "train_label_length = np.array(train_label_length)\n",
        "\n",
        "valid_img = np.array(valid_img)\n",
        "valid_input_length = np.array(valid_input_length)\n",
        "valid_label_length = np.array(valid_label_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pO9lTJYpVRf"
      },
      "outputs": [],
      "source": [
        "train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
        "valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = len(char_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEhL6S2oodQA",
        "outputId": "e7382b76-3a8b-42d3-b100-3de280520ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "11/11 [==============================] - ETA: 0s - loss: 22.8891\n",
            "Epoch 1: val_loss improved from inf to 20.65809, saving model to best_model.hdf5\n",
            "11/11 [==============================] - 19s 1s/step - loss: 22.8891 - val_loss: 20.6581\n",
            "Epoch 2/10\n",
            "11/11 [==============================] - ETA: 0s - loss: 20.4791\n",
            "Epoch 2: val_loss improved from 20.65809 to 20.31706, saving model to best_model.hdf5\n",
            "11/11 [==============================] - 10s 866ms/step - loss: 20.4791 - val_loss: 20.3171\n",
            "Epoch 3/10\n",
            "11/11 [==============================] - ETA: 0s - loss: 20.1701\n",
            "Epoch 3: val_loss did not improve from 20.31706\n",
            "11/11 [==============================] - 10s 931ms/step - loss: 20.1701 - val_loss: 20.6282\n",
            "Epoch 4/10\n",
            "11/11 [==============================] - ETA: 0s - loss: 19.9983\n",
            "Epoch 4: val_loss did not improve from 20.31706\n",
            "11/11 [==============================] - 10s 911ms/step - loss: 19.9983 - val_loss: 20.6207\n",
            "Epoch 5/10\n",
            "11/11 [==============================] - ETA: 0s - loss: 19.9408\n",
            "Epoch 5: val_loss did not improve from 20.31706\n",
            "11/11 [==============================] - 10s 921ms/step - loss: 19.9408 - val_loss: 20.4223\n",
            "Epoch 6/10\n",
            "11/11 [==============================] - ETA: 0s - loss: 19.8595\n",
            "Epoch 6: val_loss did not improve from 20.31706\n",
            "11/11 [==============================] - 10s 910ms/step - loss: 19.8595 - val_loss: 20.4277\n",
            "Epoch 7/10\n",
            "11/11 [==============================] - ETA: 0s - loss: 19.8607\n",
            "Epoch 7: val_loss did not improve from 20.31706\n",
            "11/11 [==============================] - 9s 839ms/step - loss: 19.8607 - val_loss: 20.5124\n",
            "Epoch 8/10\n",
            "11/11 [==============================] - ETA: 0s - loss: 19.7980\n",
            "Epoch 8: val_loss did not improve from 20.31706\n",
            "11/11 [==============================] - 10s 869ms/step - loss: 19.7980 - val_loss: 20.5594\n",
            "Epoch 9/10\n",
            "11/11 [==============================] - ETA: 0s - loss: 19.7071\n",
            "Epoch 9: val_loss did not improve from 20.31706\n",
            "11/11 [==============================] - 10s 908ms/step - loss: 19.7071 - val_loss: 20.5023\n",
            "Epoch 10/10\n",
            "11/11 [==============================] - ETA: 0s - loss: 19.5523\n",
            "Epoch 10: val_loss did not improve from 20.31706\n",
            "11/11 [==============================] - 10s 916ms/step - loss: 19.5523 - val_loss: 20.7410\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2596f9faf0>"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 10\n",
        "epochs = 10\n",
        "crnn.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length], y=np.zeros(len(training_img)), batch_size=batch_size, epochs = epochs, validation_data = ([valid_img, valid_padded_txt, valid_input_length, valid_label_length], [np.zeros(len(valid_img))]), verbose = 1, callbacks = callbacks_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd5qulCW1t28"
      },
      "outputs": [],
      "source": [
        "crnn_test = Image_text_recogniser_model(\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6pztrNKeLMx"
      },
      "outputs": [],
      "source": [
        "def order_points(pts):\n",
        "    rect = np.zeros((4, 2), dtype=\"float32\")\n",
        "    s = pts.sum(axis=1)\n",
        "    rect[0] = pts[np.argmin(s)]\n",
        "    rect[2] = pts[np.argmax(s)]\n",
        "    diff = np.diff(pts, axis=1)\n",
        "    rect[1] = pts[np.argmin(diff)]\n",
        "    rect[3] = pts[np.argmax(diff)]\n",
        "    return rect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdVfGZ2GeFae"
      },
      "outputs": [],
      "source": [
        "def four_point_transform(image, pts):\n",
        "    rect = order_points(pts)\n",
        "    (tl, tr, br, bl) = rect\n",
        "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
        "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
        "    maxWidth = max(int(widthA), int(widthB))\n",
        "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
        "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
        "    maxHeight = max(int(heightA), int(heightB))\n",
        "    dst = np.array([\n",
        "        [0, 0],\n",
        "        [maxWidth - 1, 0],\n",
        "        [maxWidth - 1, maxHeight - 1],\n",
        "        [0, maxHeight - 1]], dtype=\"float32\")\n",
        "    M = cv2.getPerspectiveTransform(rect, dst)\n",
        "    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
        "    return warped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG6XbBGZd3fV"
      },
      "outputs": [],
      "source": [
        "def detect(img_rgb):\n",
        "    img = img_rgb.copy()\n",
        "    input_height = img_rgb.shape[0]\n",
        "    input_width = img_rgb.shape[1]\n",
        "    hsv_frame = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # yellow color\n",
        "    low_yellow = np.array([20, 100, 100])\n",
        "    high_yellow = np.array([30, 255, 255])\n",
        "    yellow_mask = cv2.inRange(hsv_frame, low_yellow, high_yellow)\n",
        "    yellow = cv2.bitwise_and(yellow_mask, yellow_mask, mask=yellow_mask)\n",
        "\n",
        "    cv2.imwrite(\"temp/steps/1_yellow_color_detection.png\", yellow)\n",
        "    # Close morph\n",
        "    k = np.ones((5, 5), np.uint8)\n",
        "    closing = cv2.morphologyEx(yellow, cv2.MORPH_CLOSE, k)\n",
        "\n",
        "    cv2.imwrite(\"temp/steps/2_closing_morphology.png\", closing)\n",
        "    # Detect yellow area\n",
        "    contours, hierarchy = cv2.findContours(closing, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # List of final crops\n",
        "    crops = []\n",
        "\n",
        "    # Loop over contours and find license plates\n",
        "    for cnt in contours:\n",
        "        x, y, w, h = cv2.boundingRect(cnt)\n",
        "\n",
        "        # Conditions on crops dimensions and area\n",
        "        if h*6 > w > 2 * h and h > 0.1 * w and w * h > input_height * input_width * 0.0001:\n",
        "\n",
        "            # Make a crop from the RGB image, the crop is slided a bit at left to detect bleu area\n",
        "            crop_img = img_rgb[y:y + h, x-round(w/10):x]\n",
        "            crop_img = crop_img.astype('uint8')\n",
        "\n",
        "            # Compute bleu color density at the left of the crop\n",
        "            # Bleu color condition\n",
        "            try:\n",
        "                hsv_frame = cv2.cvtColor(crop_img, cv2.COLOR_BGR2HSV)\n",
        "                low_bleu = np.array([100,150,0])\n",
        "                high_bleu = np.array([140,255,255])\n",
        "                bleu_mask = cv2.inRange(hsv_frame, low_bleu, high_bleu)\n",
        "                bleu_summation = bleu_mask.sum()\n",
        "\n",
        "            except:\n",
        "                bleu_summation = 0\n",
        "\n",
        "            # Condition on bleu color density at the left of the crop\n",
        "            if bleu_summation > 550:\n",
        "\n",
        "                # Compute yellow color density in the crop\n",
        "                # Make a crop from the RGB image\n",
        "                imgray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
        "                crop_img_yellow = img_rgb[y:y + h, x:x+w]\n",
        "                crop_img_yellow = crop_img_yellow.astype('uint8')\n",
        "\n",
        "                # Detect yellow color\n",
        "                hsv_frame = cv2.cvtColor(crop_img_yellow, cv2.COLOR_BGR2HSV)\n",
        "                low_yellow = np.array([20, 100, 100])\n",
        "                high_yellow = np.array([30, 255, 255])\n",
        "                yellow_mask = cv2.inRange(hsv_frame, low_yellow, high_yellow)\n",
        "\n",
        "                # Compute yellow density\n",
        "                yellow_summation = yellow_mask.sum()\n",
        "\n",
        "                # Condition on yellow color density in the crop\n",
        "                if yellow_summation > 255*crop_img.shape[0]*crop_img.shape[0]*0.4:\n",
        "\n",
        "                    # Make a crop from the gray image\n",
        "                    crop_gray = imgray[y:y + h, x:x + w]\n",
        "                    crop_gray = crop_gray.astype('uint8')\n",
        "\n",
        "                    # Detect chars inside yellow crop with specefic dimension and area\n",
        "                    th = cv2.adaptiveThreshold(crop_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
        "                    contours2, hierarchy = cv2.findContours(th, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "                    # Init number of chars\n",
        "                    chars = 0\n",
        "                    for c in contours2:\n",
        "                        area2 = cv2.contourArea(c)\n",
        "                        x2, y2, w2, h2 = cv2.boundingRect(c)\n",
        "                        if w2 * h2 > h * w * 0.01 and h2 > w2 and area2 < h * w * 0.9:\n",
        "                            chars += 1\n",
        "\n",
        "                    # Condition on the number of chars\n",
        "                    if 20 > chars > 4:\n",
        "                        rect = cv2.minAreaRect(cnt)\n",
        "                        box = cv2.boxPoints(rect)\n",
        "                        box = np.int0(box)\n",
        "                        pts = np.array(box)\n",
        "                        warped = four_point_transform(img, pts)\n",
        "                        crops.append(warped)\n",
        "\n",
        "                        # Using cv2.putText() method\n",
        "                        #img_rgb = cv2.putText(img_rgb, 'LP', (x, y), cv2.FONT_HERSHEY_SIMPLEX,1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "                        #print(pts)\n",
        "                        cv2.drawContours(img_rgb, [box], 0, (0, 0, 255), 1)\n",
        "\n",
        "    return img_rgb, crops, pts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WBL0SJ9qbHo"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from google.colab import files\n",
        "# import keras.utils as image\n",
        "# from PIL import Image\n",
        "\n",
        "# from google.colab.patches import cv2_imshow\n",
        "\n",
        "# uploaded = files.upload()\n",
        "# print(uploaded)\n",
        "\n",
        "\n",
        "\n",
        "# img_rgb, crops, pts = detect(uploaded)\n",
        "\n",
        "# cv2_imshow(img_rgb)\n",
        "# for fn in uploaded.keys():\n",
        "#     input_image = cv2.imread(uploaded)\n",
        "#     detection, crops, box1 = detect(input_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3z9Cq7IOdEs",
        "outputId": "5c4a51fb-ec94-4dbc-cc1e-a2b8b7c93098"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f25949d1ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "original_text =   0195SM\n",
            "predicted text = 4XEN\n",
            "\n",
            "original_text =   H66544\n",
            "predicted text = 4XEN\n",
            "\n",
            "original_text =   W35478\n",
            "predicted text = 4XEN\n",
            "\n",
            "original_text =   A04129\n",
            "predicted text = 4XEN\n",
            "\n",
            "original_text =   NC6179\n",
            "predicted text = 4XEN\n",
            "\n",
            "original_text =   TB5888\n",
            "predicted text = 4XEN\n",
            "\n",
            "original_text =   2749UY\n",
            "predicted text = 4XEN\n",
            "\n",
            "original_text =   QD5532\n",
            "predicted text = 4XEN\n",
            "\n",
            "original_text =   2746BC\n",
            "predicted text = 4XEN\n",
            "\n",
            "original_text =   3561JB\n",
            "predicted text = 4XEN\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# crnn_test.load_weights('best_model.hdf5')\n",
        " \n",
        "# predict outputs on validation images\n",
        "prediction = crnn_test.predict(valid_img[:10])\n",
        "\n",
        "# use CTC decoder\n",
        "out = keras.backend.get_value(keras.backend.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        "\n",
        "# see the results\n",
        "i = 0\n",
        "for x in out:\n",
        "    print(\"original_text =  \", valid_orig_txt[i])\n",
        "    print(\"predicted text = \", end = '')\n",
        "    for p in x:  \n",
        "        if int(p) != -1:\n",
        "            print(char_list[int(p)], end = '')       \n",
        "    print('\\n')\n",
        "    i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52-W74DW8IQk"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(\"sample1.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qfiA9n5jXun",
        "outputId": "4e6dda3f-1206-4a4b-b79d-126b0f924eb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 32, 128, 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f25949d1ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "predicted text = YXYXYXZ\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# predict outputs on validation images\n",
        "# input_imgs = []\n",
        "input_img = cv2.cvtColor(cv2.imread(\"sample1.png\"), cv2.COLOR_BGR2GRAY)\n",
        "up_points = (128, 32)\n",
        "\n",
        "resized_up = cv2.resize(input_img, up_points)\n",
        "resized_up = np.expand_dims(resized_up, axis=-1)\n",
        "resized_up = np.array(resized_up)[np.newaxis, ...]\n",
        "# resized_up = np.expand_dims(resized_up, axis=0)\n",
        "print(resized_up.shape)\n",
        "prediction = crnn_test.predict(resized_up)\n",
        "\n",
        "# use CTC decoder\n",
        "out = keras.backend.get_value(keras.backend.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        "\n",
        "# see the results\n",
        "for x in out:\n",
        "    print(\"predicted text = \", end = '')\n",
        "    for p in x:\n",
        "        if int(p) != -1:\n",
        "            print(char_list[int(p)], end = '')\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69zz54FwomrW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
